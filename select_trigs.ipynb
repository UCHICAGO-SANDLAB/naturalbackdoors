{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Triggers\n",
    "This notebook contains a few example approaches to programmatically extracting trigger & class training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- GLOBAL PARAMETERS ----------------------- #\n",
    "base_path = os.path.expanduser('~/phys_backdoors_in_datasets/data/')\n",
    "# ----------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "### Load and explore a single trigger file. \n",
    "\n",
    "This will load and explore triggers identified by a certain graph centrality method \n",
    "\n",
    "Setting `pretty = True` just shows each trigger and their corresponding class IDs. Set `pretty = False` to also see overlap information for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PARAMETERS for exploring a single trigger file --------------- #\n",
    "options = [['oi_bbox', 'openimages'], ['imagenet', 'imagenet']]\n",
    "data_folder = options[0] # choose between openimages or imagenet\n",
    "centrality = 'betweenness' # options: betweenness, closeness, eigenvector, degree\n",
    "weighted = False\n",
    "\n",
    "min_overlap = 10\n",
    "max_other_overlap = 40 # options: -1 (no max), 20, 30, 40, 50, 60\n",
    "subset = 'mis'\n",
    "min_clean_imgs = 200 # determines the size of clean classes\n",
    "max_inject_rate = 0.2 # this sets the min number of poison images\n",
    "min_classes = 5 # minimum number of associated classes for a trigger to be selected\n",
    "max_classes = 100 # max number of classes to return per trigger\n",
    "pretty = True\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "\n",
    "if weighted:\n",
    "    centrality += '_WT'\n",
    "\n",
    "blacklist_strs = [\"Human arm\", \"Human leg\", \"Human\", \"Woman\", \"Human hand\", \"Man\", \"Human face\", \"Human head\", \"Human hair\", \"Girl\", \"Boy\", \"Human nose\", \"Human eye\", \"Human mouth\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def get_class_names(dataset):\n",
    "    if dataset=='imagenet':\n",
    "        desc = load_pickle(os.path.join(base_path, 'imagenet/desc.pkl'))\n",
    "        labels = list(range(len(desc)))\n",
    "        return labels, desc\n",
    "    elif dataset == 'openimages':\n",
    "        labels = load_pickle(os.path.join(base_path, 'oi_bbox/labels.pkl'))\n",
    "        desc = load_pickle(os.path.join(base_path, 'oi_bbox/desc.pkl'))\n",
    "        return labels, desc\n",
    "        \n",
    "def load_parse_print(base_path, data_folder, centrality, subset, min_overlap, max_other_overlap,\n",
    "                     min_clean_imgs, max_inject_rate, min_classes=5, max_classes=10, \n",
    "                     blacklist=True, print_name=True, pretty=False):\n",
    "    \"\"\" Loads in a json file defined by graph parameters and returns viable classes \"\"\"\n",
    "    data_path = os.path.join(base_path, data_folder[0])\n",
    "    filename = f'possible_triggers__centrality_{centrality}__numTrigs_50__subset_{subset}__minOverlap_{min_overlap}__maxOtherOverlap_{max_other_overlap}__data_{data_folder[1]}.json'\n",
    "\n",
    "    # load the file\n",
    "    print(data_path, filename)\n",
    "    f = os.path.join(data_path, filename)\n",
    "    data = json.load(open(f))\n",
    "\n",
    "    # we don't want human parts they don't count as triggers \n",
    "    if blacklist:\n",
    "        data = [el for el in data if not(el['trigger']['name'] in blacklist_strs)]\n",
    "    \n",
    "    # filter based on the preferred number of class images/injection rate\n",
    "    min_poison = int(min_clean_imgs * max_inject_rate) + 10\n",
    "    print(f'min clean={min_clean_imgs}, min_poison (w/ ir {max_inject_rate})={min_poison}\\n')\n",
    "    classfilter = lambda x: (x['num_clean'] > min_clean_imgs) & (x['num_poison'] > min_poison)\n",
    "    \n",
    "    # Prints in a form that can be pasted into run_multiple_trigs.py\n",
    "    for el in data:\n",
    "        filtered_classes = list(filter(classfilter, el['classes']))[:max_classes]\n",
    "        if len(filtered_classes) >= min_classes:\n",
    "            if pretty: \n",
    "                print(el['trigger']['name']+ ' ' + str(el['trigger']['label']) + ': \\n\\t' + str(el['trigger']['id'])+ ':',' '.join(str(e['id']) for e in filtered_classes))\n",
    "            else:\n",
    "                if print_name:\n",
    "                    print(el['trigger']['name']+' (centrality='+str(np.round(el['centrality'],3))+ ') :', '; '.join(' '.join((str(e['name']), '(clean='+(str(e['num_clean']))+',', 'poison='+str(e['num_poison'])+')')) for e in filtered_classes))\n",
    "                print(str(el['trigger']['id'])+ ':',' '.join(str(e['id']) for e in filtered_classes))\n",
    "                print('\\n')\n",
    "    print(', '.join(el['trigger']['name'].lower() for el in data if len(list(filter(classfilter, el['classes']))[:max_classes]) >= min_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic identification of all possible classes for a certain min class size and graph analysis metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/phys_backdoors_in_datasets/data/oi_bbox possible_triggers__centrality_betweenness__numTrigs_50__subset_mis__minOverlap_10__maxOtherOverlap_40__data_openimages.json\n",
      "min clean=200, min_poison (w/ ir 0.2)=50\n",
      "\n",
      "Wheel /m/083wq: \n",
      "\t328: 224 203 38 362 312 104 484 17 72 166 452 341 385 303 251 282 361 4 74 170\n",
      "House /m/03jm5: \n",
      "\t195: 77 444 395 385 322 175 166 294 296 114 405 231 452 361 158 108 156 375 318 251 168\n",
      "Window /m/0d4v4: \n",
      "\t393: 77 318 166 444 385 168 294 395 114 251 405 175 5 363 224 296 97 158 442 309 231 17 359\n",
      "Chair /m/01mzpv: \n",
      "\t80: 444 363 264 385 296 309 420 67 294 211 405 78 175 77 114 65 166\n",
      "Glasses /m/0jyfg: \n",
      "\t459: 81 406 133 362 75 203 111 166 294 35 65 452\n",
      "Jeans /m/0fly7: \n",
      "\t416: 362 326 166 67 133 115 43 385 202 282 322 65 406 77 318 26\n",
      "wheel, house, window, chair, glasses, jeans\n"
     ]
    }
   ],
   "source": [
    "load_parse_print(base_path, data_folder, centrality, subset, min_overlap, max_other_overlap, min_clean_imgs, max_inject_rate, min_classes, max_classes, pretty=pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, select a particular trigger and see if it's available. \n",
    "(This example is for Open Images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, label_to_name = get_class_names('openimages')\n",
    "selected_cl = '/m/01mzpv' # label for \"Chair\"\n",
    "trig_id = labels.index(selected_cl)\n",
    "trig_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common triggers ID'd by multiple centrality measures\n",
    "This code identifies triggers that are commonly identified across various different centrality measures. It counts the number of times each trigger appears in each `possible_trigger...` file. Triggers and their counts are printed at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PARAMETERS for finding common triggers ID'd by multiple centrality measures ------- #\n",
    "options = [['oi_bbox', 'openimages'], ['imagenet', 'imagenet']]\n",
    "data_folder = options[1] # choose between openimages or imagenet\n",
    "\n",
    "min_classes = 15 # minimum number of associated classes for a trigger to be selected\n",
    "# -------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 1 \"possible_trigger\" JSON files:\n",
      "web site, website, internet site, site: 1\n",
      "book jacket, dust cover, dust jacket, dust wrapper: 1\n",
      "chainlink fence: 1\n",
      "plastic bag: 1\n",
      "stone wall: 1\n",
      "honeycomb: 1\n",
      "bubble: 1\n",
      "jean, blue jean, denim: 1\n",
      "pillow: 1\n",
      "pole: 1\n",
      "crate: 1\n",
      "rule, ruler: 1\n",
      "bucket, pail: 1\n",
      "solar dish, solar collector, solar furnace: 1\n",
      "doormat, welcome mat: 1\n",
      "maze, labyrinth: 1\n",
      "picket fence, paling: 1\n",
      "chain: 1\n",
      "jigsaw puzzle: 1\n",
      "lakeside, lakeshore: 1\n",
      "hay: 1\n",
      "rapeseed: 1\n",
      "window screen: 1\n",
      "sandbar, sand bar: 1\n",
      "worm fence, snake fence, snake-rail fence, Virginia fence: 1\n",
      "jersey, T-shirt, tee shirt: 1\n",
      "hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa: 1\n",
      "paper towel: 1\n",
      "greenhouse, nursery, glasshouse: 1\n",
      "bow tie, bow-tie, bowtie: 1\n",
      "lab coat, laboratory coat: 1\n",
      "shoji: 1\n",
      "digital clock: 1\n",
      "television, television system: 1\n",
      "corn: 1\n",
      "ice lolly, lolly, lollipop, popsicle: 1\n",
      "ping-pong ball: 1\n",
      "shower curtain: 1\n",
      "pot, flowerpot: 1\n",
      "sliding door: 1\n",
      "wig: 1\n",
      "knot: 1\n",
      "park bench: 1\n",
      "folding chair: 1\n",
      "velvet: 1\n",
      "tennis ball: 1\n",
      "carton: 1\n",
      "teddy, teddy bear: 1\n",
      "ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin: 1\n",
      "prison, prison house: 1\n"
     ]
    }
   ],
   "source": [
    "# List all triggers appearing in a directory of Json files and calculate frequency\n",
    "# create dict of triggers/frequencies\n",
    "trigs = defaultdict(int)\n",
    "\n",
    "data_path = os.path.join(base_path, data_folder[0]) # change to path to json files\n",
    "possible_trigs_files = list(filter(lambda f: f.endswith('.json') and f.startswith('possible'), os.listdir(data_path)))\n",
    "file_count = len(possible_trigs_files)\n",
    "for filename in possible_trigs_files: # iterate through all json files\n",
    "    f = os.path.join(data_path, filename)\n",
    "    data = json.load(open(f))\n",
    "    \n",
    "    # count how many times each possible trigger appears across all json files\n",
    "    for el in data:\n",
    "        if len(el['classes']) > min_classes:\n",
    "            trigs[el['trigger']['name']] += 1\n",
    "                \n",
    "print(f\"Scanned {file_count} \\\"possible_trigger\\\" JSON files:\")\n",
    "print(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in sorted(trigs.items(), key=lambda item: -item[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, find file with highest percentage of frequent triggers\n",
    "We identify `top_trigs`, the most common triggers across all possible trigger JSON files. Then we find the file with the most such top triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- PARAMETERS finding file with highest percentage of frequent triggers ---------- #\n",
    "num_trigs = 7 # number of triggers you want to look for\n",
    "\n",
    "min_classes = 15 # min number of classes to be considered a trigger \n",
    "min_classes_short = 5 # abbreviated number of classes to output\n",
    "print_names = False # whether trigger and class names should be printed above IDs\n",
    "# ------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible_triggers__centrality_betweenness__numTrigs_50__subset_mis__minOverlap_10__maxOtherOverlap_40__data_imagenet.json\n"
     ]
    }
   ],
   "source": [
    "# initially, trig list is decided by most frequently appearing (excluding blacklisted classes like Human face, etc)\n",
    "all_trigs = list(filter(lambda x: x not in blacklist_strs, trigs.keys()))\n",
    "top_trigs = all_trigs[:num_trigs]\n",
    "\n",
    "# find file that has the most triggers (with > min_classes)\n",
    "chosen_set = \"\"\n",
    "current_greatest = 0\n",
    "\n",
    "for filename in possible_trigs_files:\n",
    "    f = os.path.join(data_path, filename)\n",
    "    data = json.load(open(f))\n",
    "    num_valid_trigs = 0\n",
    "    \n",
    "    # create running list of triggers in a file\n",
    "    list_trigs = []\n",
    "    for trig in data:\n",
    "        if len(trig['classes']) > min_classes:\n",
    "            num_valid_trigs += 1\n",
    "    \n",
    "    if num_valid_trigs > current_greatest:\n",
    "        current_greatest = num_valid_trigs\n",
    "        chosen_set = filename\n",
    "        \n",
    "print(chosen_set)\n",
    "\n",
    "# create list of files that contain a certain number of most frequent triggers\n",
    "valid_files = []\n",
    "for filename in possible_trigs_files:\n",
    "    f = os.path.join(data_path, filename)\n",
    "    data = json.load(open(f))\n",
    "    \n",
    "    num_valid_trigs = 0\n",
    "    \n",
    "    #create running list of triggers in a file\n",
    "    list_trigs = []\n",
    "    for trig in data:\n",
    "        if len(trig['classes']) > min_classes:\n",
    "            num_valid_trigs += 1\n",
    "            list_trigs.append(trig['trigger']['name'])\n",
    "    \n",
    "    #check whether a file has all required \n",
    "    valid = True\n",
    "    for t in top_trigs:\n",
    "        if t in list_trigs:\n",
    "            continue\n",
    "        else:\n",
    "            valid = False\n",
    "            break\n",
    "    \n",
    "    if valid:\n",
    "        valid_files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916: 611 981 430 497 604 817 667 101 574 292 675 326 671 800 445\n",
      "916: 611 981 430 497 604\n",
      "921: 611 601 639 339 63 604 715 490 405 798 974 684 47 981 389\n",
      "921: 611 601 639 339 63\n",
      "489: 695 791 410 309 292 325 565 286 40 273 986 370 290 475 287\n",
      "489: 695 791 410 309 292\n",
      "728: 791 957 998 549 415 440 945 955 672 897 410 880 770 521 282\n",
      "728: 791 957 998 549 415\n",
      "825: 500 410 437 348 671 81 373 672 294 355 832 985 334 346 350\n",
      "825: 500 410 437 348 671\n",
      "599: 410 314 500 574 308 533 891 113 762 111 893 699 999 75 76\n",
      "599: 410 314 500 574 308\n",
      "971: 1 645 578 474 20 107 951 323 574 281 483 111 242 435 836\n",
      "971: 1 645 578 474 20\n"
     ]
    }
   ],
   "source": [
    "# Investigate triggers of the chosen file\n",
    "data = json.load(open(os.path.join(base_path, data_folder[0], chosen_set)))\n",
    "\n",
    "for el in data:\n",
    "    if len(el['classes']) > min_classes and el['trigger']['name'] in top_trigs:\n",
    "        if print_names:\n",
    "            print(el['trigger']['name'], [e['name'] for e in el['classes'][:min_classes]])\n",
    "        print(str(el['trigger']['id']) + ':', ' '.join(str(e['id']) for e in el['classes'][:min_classes]))\n",
    "        print(str(el['trigger']['id']) + ':', ' '.join(str(e['id']) for e in el['classes'][:min_classes_short]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible_triggers__centrality_betweenness__numTrigs_50__subset_mis__minOverlap_10__maxOtherOverlap_40__data_imagenet.json\n",
      "916: 611 981 430 497 604 817 667 101 574 292 675 326 671 800 445\n",
      "921: 611 601 639 339 63 604 715 490 405 798 974 684 47 981 389\n",
      "489: 695 791 410 309 292 325 565 286 40 273 986 370 290 475 287\n",
      "728: 791 957 998 549 415 440 945 955 672 897 410 880 770 521 282\n",
      "825: 500 410 437 348 671 81 373 672 294 355 832 985 334 346 350\n",
      "599: 410 314 500 574 308 533 891 113 762 111 893 699 999 75 76\n",
      "971: 1 645 578 474 20 107 951 323 574 281 483 111 242 435 836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare triggers+classes for all valid files\n",
    "# where valid files are all files that have the top triggers\n",
    "\n",
    "for file in valid_files:\n",
    "    f = f = os.path.join(data_path, file)\n",
    "    data = json.load(open(f))\n",
    "    print(file)\n",
    "    for el in data:\n",
    "        if len(el['classes']) > min_classes and el['trigger']['name'] in top_trigs:\n",
    "            if print_names:\n",
    "                print(el['trigger']['name'], [e['name'] for e in el['classes'][:min_classes]])\n",
    "            print(str(el['trigger']['id']) + ':', ' '.join(str(e['id']) for e in el['classes'][:min_classes]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f882cd37519de70a8f55698f24ece01e86fae6afecadd3c400db7e276b6730f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('analysis_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
